{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preparation\n"
      ],
      "metadata": {
        "id": "k3KoOrJazovd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIyByzqgzGnS"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------\n",
        "# a) Torch imports\n",
        "# -------------------------------------------\n",
        "import os, glob, re, random, math, copy, time\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "\n",
        "# -------------------------------------------\n",
        "# b) label‑making rule  (adapted thresholds)\n",
        "# -------------------------------------------\n",
        "def to_label(ctr, impr):\n",
        "    if impr < 1e5 and ctr < 0.10:      # low\n",
        "        return 0\n",
        "    if impr >= 3e5 or ctr >= 0.20:     # high\n",
        "        return 2\n",
        "    return 1                           # medium\n",
        "\n",
        "# -------------------------------------------\n",
        "# c) ordinary image dataset\n",
        "# -------------------------------------------\n",
        "class AdImageDataset(Dataset):\n",
        "    _pattern = re.compile(r'([\\d.]+)_([\\d]+)_.+\\.jpg$', re.I)\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.fpaths = glob.glob(os.path.join(root, '*.jpg'))\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        for p in self.fpaths:\n",
        "            m = self._pattern.search(os.path.basename(p))\n",
        "            if not m: continue\n",
        "            ctr  = float(m.group(1))\n",
        "            impr = float(m.group(2))\n",
        "            label = to_label(ctr, impr)\n",
        "            self.samples.append((p, label))\n",
        "\n",
        "        self.labels  = [lbl for _, lbl in self.samples]\n",
        "        self.indices_to_labels = lambda idx: [self.labels[i]\n",
        "                                              for i in idx]\n",
        "    def __len__(self):  return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        if self.transform:  img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# common transform\n",
        "img_tf = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
        "                         std=[0.229,0.224,0.225])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install --no-deps torchmeta==1.8.0\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vjX3fOq1s18",
        "outputId": "667cf7b7-7172-4fda-bc75-538516ad0489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmeta==1.8.0\n",
            "  Downloading torchmeta-1.8.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Downloading torchmeta-1.8.0-py3-none-any.whl (210 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/210.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.4/210.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchmeta\n",
            "Successfully installed torchmeta-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ordered-set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2fX1hXh4mke",
        "outputId": "f4387544-84b8-48ee-eed1-84400fbfa382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ordered-set\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: ordered-set\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchmeta 1.8.0 requires torch<1.10.0,>=1.4.0, but you have torch 2.6.0+cu124 which is incompatible.\n",
            "torchmeta 1.8.0 requires torchvision<0.11.0,>=0.5.0, but you have torchvision 0.21.0+cu124 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ordered-set-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets.utils as tv_utils\n",
        "\n",
        "# torchmeta 1.x expects these, but newer torchvision has removed them\n",
        "def _get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "    return None\n",
        "\n",
        "def _save_response_content(response, destination, chunk_size=32768):\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(chunk_size):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "# Monkey-patch them into torchvision\n",
        "tv_utils._get_confirm_token    = _get_confirm_token\n",
        "tv_utils._save_response_content = _save_response_content\n"
      ],
      "metadata": {
        "id": "zBblqfbM4_q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content -maxdepth 2 -type d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjJ0MpdO6Q5u",
        "outputId": "34c71640-15de-42e9-d3b1-7fedd4e4b0bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/.config\n",
            "/content/.config/logs\n",
            "/content/.config/configurations\n",
            "/content/sample_data\n",
            "/content/sample_data/.ipynb_checkpoints\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prototypical Network (episodic training, TorchMeta)\n"
      ],
      "metadata": {
        "id": "f3mT73kEzrMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmeta.utils.data.task import Dataset as TMTaskDataset\n",
        "\n",
        "def __getitem__(self, class_idx):\n",
        "    paths = self.class_to_imgs[self.classes[class_idx]]\n",
        "\n",
        "    class OneClassDataset(TMTaskDataset):\n",
        "        def __init__(self, paths, transform, index):\n",
        "                # call the torchmeta Dataset constructor so\n",
        "                # .index, .transform and .target_transform_append are there\n",
        "                super().__init__(index, transform=transform, target_transform=None)\n",
        "                self.paths = paths\n",
        "\n",
        "        def __len__(self):\n",
        "                return len(self.paths)\n",
        "\n",
        "        def __getitem__(self, i):\n",
        "                img = Image.open(self.paths[i]).convert('RGB')\n",
        "                #  transform already on self.transform\n",
        "                return self.transform(img), class_idx\n",
        "\n",
        "    return OneClassDataset(paths, self.transform, class_idx)\n"
      ],
      "metadata": {
        "id": "KMfJZZSUF_DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── Imports & setup ────────────────────────────────────────\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torchmeta.utils.data import ClassDataset, CombinationMetaDataset, BatchMetaDataLoader\n",
        "from torchmeta.transforms import ClassSplitter\n",
        "from torchmeta.utils.data.task import Dataset as TMTaskDataset\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "N_way, K_shot, Q_query = 3, 5, 10\n",
        "num_epochs = 20\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── AdClassDataset subclass ───────────────────────────────────────────\n",
        "class AdClassDataset(ClassDataset):\n",
        "    def __init__(self, root, transform, meta_train=True):\n",
        "        super().__init__(meta_train=meta_train)\n",
        "        self.transform = transform\n",
        "        base = AdImageDataset(root, transform)\n",
        "        groups = defaultdict(list)\n",
        "        for p, l in base.samples:\n",
        "            groups[l].append(p)\n",
        "        self.classes = list(groups.keys())\n",
        "        self.class_to_imgs = groups\n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        return len(self.classes)\n",
        "    def __len__(self):\n",
        "        return self.num_classes\n",
        "    def __getitem__(self, class_idx):\n",
        "        paths = self.class_to_imgs[self.classes[class_idx]]\n",
        "        class OneClassDataset(TMTaskDataset):\n",
        "            def __init__(self, paths, transform, index):\n",
        "                super().__init__(index, transform=transform, target_transform=None)\n",
        "                self.paths = paths\n",
        "            def __len__(self):\n",
        "                return len(self.paths)\n",
        "            def __getitem__(self, i):\n",
        "                img = Image.open(self.paths[i]).convert('RGB')\n",
        "                return self.transform(img), class_idx\n",
        "        return OneClassDataset(paths, self.transform, class_idx)\n",
        "\n",
        "# ─── Build meta‐dataset & loader ────────────────────────────────────────────\n",
        "dataset_transform = ClassSplitter(shuffle=True,\n",
        "                                 num_train_per_class=K_shot,\n",
        "                                 num_test_per_class=Q_query)\n",
        "meta_ds = CombinationMetaDataset(\n",
        "    AdClassDataset('/content/sample_data', img_tf),\n",
        "    num_classes_per_task=N_way,\n",
        "    dataset_transform=dataset_transform\n",
        ")\n",
        "loader = BatchMetaDataLoader(meta_ds,\n",
        "    batch_size=1, shuffle=True, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "# ─── ProtoNet & optimizer ──────────────────────────────────────────────────\n",
        "class ProtoNet(nn.Module):\n",
        "    def __init__(self, emb_dim=512):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "        self.backbone.fc = nn.Identity()\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "model = ProtoNet().to(device)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# ─── Training loop with best‐model tracking ─────────────────────────────────\n",
        "best_r2 = float('-inf')\n",
        "best_metrics = {}\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_trues, all_preds = [], []\n",
        "\n",
        "    for batch in loader:\n",
        "        # unpack and reshape\n",
        "        (support_x, support_y) = batch['train']\n",
        "        ( query_x,   query_y) = batch['test']\n",
        "        support_x = support_x.squeeze(0).to(device)\n",
        "        support_y = support_y.squeeze(0).to(device)\n",
        "        query_x   = query_x.squeeze(0).to(device)\n",
        "        query_y   = query_y.squeeze(0).to(device)\n",
        "\n",
        "        # forward / loss\n",
        "        emb_sup = model(support_x)\n",
        "        emb_qry = model(query_x)\n",
        "        protos = torch.stack([emb_sup[support_y==c].mean(0)\n",
        "                               for c in range(N_way)])\n",
        "        dists  = ((emb_qry.unsqueeze(1)-protos)**2).sum(-1)\n",
        "        logits = -dists\n",
        "        loss   = F.cross_entropy(logits, query_y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        preds = logits.argmax(1).cpu().tolist()\n",
        "        trues = query_y.cpu().tolist()\n",
        "        all_preds.extend(preds)\n",
        "        all_trues.extend(trues)\n",
        "\n",
        "    # compute metrics this epoch\n",
        "    mae  = mean_absolute_error(all_trues, all_preds)\n",
        "    mse  = mean_squared_error(all_trues, all_preds)\n",
        "    rmse = mse**0.5\n",
        "    r2   = r2_score(all_trues, all_preds)\n",
        "    avg_loss = running_loss / len(loader)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} — loss {avg_loss:.4f}  \"\n",
        "          f\"MAE {mae:.4f}  MSE {mse:.4f}  RMSE {rmse:.4f}  R² {r2:.4f}\")\n",
        "\n",
        "    # track best by highest R²\n",
        "    if r2 > best_r2:\n",
        "        best_r2 = r2\n",
        "        best_metrics = dict(MAE=mae, MSE=mse, RMSE=rmse, R2=r2, loss=avg_loss)\n",
        "        best_state = model.state_dict()\n",
        "\n",
        "# ─── Final-best metrics ─────────────────────────────────────────────────────\n",
        "print(\"\\n>>> Best model metrics:\")\n",
        "print(f\" MAE:  {best_metrics['MAE']:.4f}\")\n",
        "print(f\" MSE:  {best_metrics['MSE']:.4f}\")\n",
        "print(f\" RMSE: {best_metrics['RMSE']:.4f}\")\n",
        "print(f\" R²:   {best_metrics['R2']:.4f}\")\n",
        "print(f\" loss: {best_metrics['loss']:.4f}\")\n",
        "\n",
        "# load best weights back into the model\n",
        "model.load_state_dict(best_state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEsxnJL2FOKM",
        "outputId": "61dd9057-8e08-4779-d720-c921bad3e265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 116MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 — loss 14.6464  MAE 0.5333  MSE 0.8667  RMSE 0.9309  R² -0.3000\n",
            "Epoch 02 — loss 21.1344  MAE 0.7333  MSE 1.2000  RMSE 1.0954  R² -0.8000\n",
            "Epoch 03 — loss 2.6205  MAE 0.1000  MSE 0.1000  RMSE 0.3162  R² 0.8500\n",
            "Epoch 04 — loss 8.5375  MAE 0.5000  MSE 0.8333  RMSE 0.9129  R² -0.2500\n",
            "Epoch 05 — loss 9.8611  MAE 0.5000  MSE 0.7667  RMSE 0.8756  R² -0.1500\n",
            "Epoch 06 — loss 0.9096  MAE 0.1667  MSE 0.2333  RMSE 0.4830  R² 0.6500\n",
            "Epoch 07 — loss 6.8392  MAE 0.3333  MSE 0.4667  RMSE 0.6831  R² 0.3000\n",
            "Epoch 08 — loss 4.9625  MAE 0.0333  MSE 0.0333  RMSE 0.1826  R² 0.9500\n",
            "Epoch 09 — loss 1.6025  MAE 0.1333  MSE 0.2000  RMSE 0.4472  R² 0.7000\n",
            "Epoch 10 — loss 6.1206  MAE 0.2333  MSE 0.3000  RMSE 0.5477  R² 0.5500\n",
            "Epoch 11 — loss 2.1177  MAE 0.1000  MSE 0.1667  RMSE 0.4082  R² 0.7500\n",
            "Epoch 12 — loss 5.2206  MAE 0.2333  MSE 0.4333  RMSE 0.6583  R² 0.3500\n",
            "Epoch 13 — loss 0.0004  MAE 0.0000  MSE 0.0000  RMSE 0.0000  R² 1.0000\n",
            "Epoch 14 — loss 0.0004  MAE 0.0000  MSE 0.0000  RMSE 0.0000  R² 1.0000\n",
            "Epoch 15 — loss 0.0007  MAE 0.0000  MSE 0.0000  RMSE 0.0000  R² 1.0000\n",
            "Epoch 16 — loss 0.0062  MAE 0.0000  MSE 0.0000  RMSE 0.0000  R² 1.0000\n",
            "Epoch 17 — loss 0.0000  MAE 0.0000  MSE 0.0000  RMSE 0.0000  R² 1.0000\n",
            "Epoch 18 — loss 0.0000  MAE 0.0000  MSE 0.0000  RMSE 0.0000  R² 1.0000\n",
            "Epoch 19 — loss 0.2203  MAE 0.0667  MSE 0.1333  RMSE 0.3651  R² 0.8000\n",
            "Epoch 20 — loss 0.0000  MAE 0.0000  MSE 0.0000  RMSE 0.0000  R² 1.0000\n",
            "\n",
            ">>> Best model metrics:\n",
            " MAE:  0.0000\n",
            " MSE:  0.0000\n",
            " RMSE: 0.0000\n",
            " R²:   1.0000\n",
            " loss: 0.0004\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = AdClassDataset('/content/sample_data', img_tf)\n",
        "\n",
        "# 1) Number of classes\n",
        "print(f\"Total classes: {dataset.num_classes}\")\n",
        "\n",
        "# 2) Samples per class\n",
        "total = 0\n",
        "for cls in dataset.classes:\n",
        "    n = len(dataset.class_to_imgs[cls])\n",
        "    print(f\"  Class {cls!r} has {n} samples\")\n",
        "    total += n\n",
        "\n",
        "print(f\"Total samples across all classes: {total}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QiW14fjVQOK",
        "outputId": "d75f8c0e-cc6f-4df8-b816-d2353cea17a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total classes: 3\n",
            "  Class 2 has 487 samples\n",
            "  Class 1 has 142 samples\n",
            "  Class 0 has 28 samples\n",
            "Total samples across all classes: 657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_PfIZona5tqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAML (Model‑Agnostic Meta‑Learning with learn2learn)"
      ],
      "metadata": {
        "id": "OxuB9g1hz4Oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools wheel cython\n",
        "!git clone https://github.com/learnables/learn2learn.git\n",
        "%cd learn2learn\n",
        "!pip install -e .\n",
        "%cd ..\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ISv7EwpsX5A",
        "outputId": "9f797125-7ab1-4c3f-d225-d55ff228ac8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.8.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "fatal: destination path 'learn2learn' already exists and is not an empty directory.\n",
            "/content/learn2learn\n",
            "Obtaining file:///content/learn2learn\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.11/dist-packages (from learn2learn==0.2.1) (2.0.2)\n",
            "Requirement already satisfied: gym>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from learn2learn==0.2.1) (0.25.2)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from learn2learn==0.2.1) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from learn2learn==0.2.1) (0.21.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from learn2learn==0.2.1) (1.15.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from learn2learn==0.2.1) (2.32.3)\n",
            "Requirement already satisfied: gsutil in /usr/local/lib/python3.11/dist-packages (from learn2learn==0.2.1) (5.34)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from learn2learn==0.2.1) (4.67.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym>=0.14.0->learn2learn==0.2.1) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym>=0.14.0->learn2learn==0.2.1) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->learn2learn==0.2.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.1.0->learn2learn==0.2.1) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.3.0->learn2learn==0.2.1) (11.2.1)\n",
            "Requirement already satisfied: argcomplete>=3.5.1 in /usr/local/lib/python3.11/dist-packages (from gsutil->learn2learn==0.2.1) (3.6.2)\n",
            "Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python3.11/dist-packages (from gsutil->learn2learn==0.2.1) (1.7)\n",
            "Requirement already satisfied: fasteners>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from gsutil->learn2learn==0.2.1) (0.19)\n",
            "Requirement already satisfied: gcs-oauth2-boto-plugin>=3.2 in /usr/local/lib/python3.11/dist-packages (from gsutil->learn2learn==0.2.1) (3.2)\n",
            "Requirement already satisfied: google-apitools>=0.5.32 in /usr/local/lib/python3.11/dist-packages (from gsutil->learn2learn==0.2.1) (0.5.32)\n",
            "Requirement already satisfied: httplib2==0.20.4 in /usr/local/lib/python3.11/dist-packages (from gsutil->learn2learn==0.2.1) (0.20.4)\n",
            "Requirement already satisfied: google-reauth>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from gsutil->learn2learn==0.2.1) (0.1.1)\n",
            "Requirement already satisfied: monotonic>=1.4 in /usr/local/lib/python3.11/dist-packages (from gsutil->learn2learn==0.2.1) (1.6)\n",
            "Requirement already satisfied: pyOpenSSL<=24.2.1,>=0.13 in /usr/local/lib/python3.11/dist-packages (from gsutil->learn2learn==0.2.1) (24.2.1)\n",
            "Requirement already satisfied: retry-decorator>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gsutil->learn2learn==0.2.1) (1.1.1)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from gsutil->learn2learn==0.2.1) (1.17.0)\n",
            "Requirement already satisfied: google-auth==2.17.0 in /usr/local/lib/python3.11/dist-packages (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.1) (2.17.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from gsutil->learn2learn==0.2.1) (0.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.1) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.1) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.1) (4.7.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0dev,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.1) (3.11.15)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2==0.20.4->gsutil->learn2learn==0.2.1) (3.2.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.1) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.1) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.1) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.1) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.1) (1.20.0)\n",
            "Requirement already satisfied: cryptography<44,>=41.0.5 in /usr/local/lib/python3.11/dist-packages (from pyOpenSSL<=24.2.1,>=0.13->gsutil->learn2learn==0.2.1) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<44,>=41.0.5->pyOpenSSL<=24.2.1,>=0.13->gsutil->learn2learn==0.2.1) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->learn2learn==0.2.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->learn2learn==0.2.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->learn2learn==0.2.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->learn2learn==0.2.1) (2025.4.26)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from rsa<5,>=3.1.4->google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.1) (0.6.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<44,>=41.0.5->pyOpenSSL<=24.2.1,>=0.13->gsutil->learn2learn==0.2.1) (2.22)\n",
            "Requirement already satisfied: boto>=2.29.1 in /usr/local/lib/python3.11/dist-packages (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn==0.2.1) (2.49.0)\n",
            "Requirement already satisfied: oauth2client>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn==0.2.1) (4.1.3)\n",
            "Requirement already satisfied: pyu2f in /usr/local/lib/python3.11/dist-packages (from google-reauth>=0.1.0->gsutil->learn2learn==0.2.1) (0.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.1.0->learn2learn==0.2.1) (3.0.2)\n",
            "Installing collected packages: learn2learn\n",
            "  Attempting uninstall: learn2learn\n",
            "    Found existing installation: learn2learn 0.2.1\n",
            "    Uninstalling learn2learn-0.2.1:\n",
            "      Successfully uninstalled learn2learn-0.2.1\n",
            "\u001b[33m  DEPRECATION: Legacy editable install of learn2learn==0.2.1 from file:///content/learn2learn (setup.py develop) is deprecated. pip 25.3 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running setup.py develop for learn2learn\n",
            "Successfully installed learn2learn-0.2.1\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import copy\n",
        "\n",
        "# Custom MAML implementation\n",
        "class MAML:\n",
        "    def __init__(self, model, lr=1e-3, first_order=True):\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.first_order = first_order\n",
        "\n",
        "    def clone(self):\n",
        "        # Create a deep copy of the model\n",
        "        cloned_model = copy.deepcopy(self.model)\n",
        "        return MAMLLearner(cloned_model, self.lr, self.first_order)\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.model.parameters()\n",
        "\n",
        "class MAMLLearner:\n",
        "    def __init__(self, model, lr, first_order):\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.first_order = first_order\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def adapt(self, loss):\n",
        "        # Compute gradients\n",
        "        grads = torch.autograd.grad(\n",
        "            loss,\n",
        "            self.model.parameters(),\n",
        "            create_graph=not self.first_order,\n",
        "            retain_graph=not self.first_order\n",
        "        )\n",
        "\n",
        "        # Update parameters\n",
        "        for param, grad in zip(self.model.parameters(), grads):\n",
        "            if grad is not None:\n",
        "                param.data = param.data - self.lr * grad\n",
        "\n",
        "# Manual task creation approach\n",
        "class ManualTaskSampler:\n",
        "    def __init__(self, dataset, n_ways=3, k_shots=5, query_shots=10):\n",
        "        self.dataset = dataset\n",
        "        self.n_ways = n_ways\n",
        "        self.k_shots = k_shots\n",
        "        self.query_shots = query_shots\n",
        "\n",
        "        # Group samples by class\n",
        "        self.class_indices = defaultdict(list)\n",
        "        for idx in range(len(dataset)):\n",
        "            _, label = dataset[idx]\n",
        "            self.class_indices[label].append(idx)\n",
        "\n",
        "        self.classes = list(self.class_indices.keys())\n",
        "        print(f\"Found {len(self.classes)} classes with samples: {[len(v) for v in self.class_indices.values()]}\")\n",
        "\n",
        "        # Check if we have enough classes\n",
        "        if len(self.classes) < self.n_ways:\n",
        "            raise ValueError(f\"Dataset has only {len(self.classes)} classes, but n_ways={self.n_ways}\")\n",
        "\n",
        "    def sample_task(self):\n",
        "        # Randomly select n_ways classes\n",
        "        selected_classes = random.sample(self.classes, self.n_ways)\n",
        "\n",
        "        support_data, support_labels = [], []\n",
        "        query_data, query_labels = [], []\n",
        "\n",
        "        for new_label, original_class in enumerate(selected_classes):\n",
        "            # Get all indices for this class\n",
        "            class_indices = self.class_indices[original_class]\n",
        "\n",
        "            # Sample k_shots + query_shots examples\n",
        "            n_samples_needed = self.k_shots + self.query_shots\n",
        "            if len(class_indices) < n_samples_needed:\n",
        "                # If not enough samples, sample with replacement\n",
        "                sampled_indices = random.choices(class_indices, k=n_samples_needed)\n",
        "            else:\n",
        "                sampled_indices = random.sample(class_indices, n_samples_needed)\n",
        "\n",
        "            # Split into support and query\n",
        "            support_indices = sampled_indices[:self.k_shots]\n",
        "            query_indices = sampled_indices[self.k_shots:self.k_shots + self.query_shots]\n",
        "\n",
        "            # Get support data\n",
        "            for idx in support_indices:\n",
        "                data, _ = self.dataset[idx]\n",
        "                support_data.append(data)\n",
        "                support_labels.append(new_label)  # remapped label\n",
        "\n",
        "            # Get query data\n",
        "            for idx in query_indices:\n",
        "                data, _ = self.dataset[idx]\n",
        "                query_data.append(data)\n",
        "                query_labels.append(new_label)  # remapped label\n",
        "\n",
        "        # Convert to tensors\n",
        "        support_data = torch.stack(support_data)\n",
        "        support_labels = torch.tensor(support_labels)\n",
        "        query_data = torch.stack(query_data)\n",
        "        query_labels = torch.tensor(query_labels)\n",
        "\n",
        "        # Combine support and query\n",
        "        all_data = torch.cat([support_data, query_data], dim=0)\n",
        "        all_labels = torch.cat([support_labels, query_labels], dim=0)\n",
        "\n",
        "        return all_data, all_labels\n",
        "\n",
        "# Usage with the dataset\n",
        "base_ds = AdImageDataset('/content/sample_data', img_tf)\n",
        "task_sampler = ManualTaskSampler(base_ds, n_ways=3, k_shots=5, query_shots=10)\n",
        "\n",
        "# Define model\n",
        "class ConvClassifier(nn.Module):\n",
        "    def __init__(self, n_out=3):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet18(weights=None)\n",
        "        self.backbone.fc = nn.Linear(512, n_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ConvClassifier().to(device)\n",
        "\n",
        "# Wrap in MAML\n",
        "maml = MAML(model, lr=1e-3, first_order=True)\n",
        "opt = torch.optim.Adam(maml.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"Starting meta-training with custom MAML implementation...\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, 1001):\n",
        "    try:\n",
        "        # Sample a task manually\n",
        "        data, labels = task_sampler.sample_task()\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "        # Split into support and query\n",
        "        n_support = 3 * 5  # n_ways * k_shots\n",
        "        support_x, support_y = data[:n_support], labels[:n_support]\n",
        "        query_x, query_y = data[n_support:], labels[n_support:]\n",
        "\n",
        "        # MAML adaptation\n",
        "        learner = maml.clone()\n",
        "        support_logits = learner(support_x)\n",
        "        loss_s = F.cross_entropy(support_logits, support_y)\n",
        "        learner.adapt(loss_s)\n",
        "\n",
        "        # Query loss\n",
        "        query_logits = learner(query_x)\n",
        "        loss_q = F.cross_entropy(query_logits, query_y)\n",
        "\n",
        "        # Meta-update\n",
        "        opt.zero_grad()\n",
        "        loss_q.backward()\n",
        "        opt.step()\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            with torch.no_grad():\n",
        "                preds = query_logits.argmax(dim=1).cpu().numpy()\n",
        "                trues = query_y.cpu().numpy()\n",
        "\n",
        "                acc = (preds == trues).mean()\n",
        "                mae = mean_absolute_error(trues, preds)\n",
        "                mse = mean_squared_error(trues, preds)\n",
        "                rmse = np.sqrt(mse)\n",
        "                r2 = r2_score(trues, preds)\n",
        "\n",
        "                print(f\"Epoch {epoch:4d}  Loss {loss_q.item():.4f}  \"\n",
        "                      f\"Acc {acc:.3f}  MAE {mae:.4f}  MSE {mse:.4f}  \"\n",
        "                      f\"RMSE {rmse:.4f}  R² {r2:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error at epoch {epoch}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        break\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Function to evaluate on new tasks\n",
        "def evaluate_few_shot(maml, task_sampler, n_test_tasks=50):\n",
        "    \"\"\"Evaluate the meta-learned model on new tasks\"\"\"\n",
        "    test_accuracies = []\n",
        "\n",
        "    print(f\"Evaluating on {n_test_tasks} test tasks...\")\n",
        "\n",
        "    for i in range(n_test_tasks):\n",
        "        try:\n",
        "            # Sample a test task\n",
        "            data, labels = task_sampler.sample_task()\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "            n_support = 3 * 5\n",
        "            support_x, support_y = data[:n_support], labels[:n_support]\n",
        "            query_x, query_y = data[n_support:], labels[n_support:]\n",
        "\n",
        "            # Clone and adapt\n",
        "            learner = maml.clone()\n",
        "            support_logits = learner(support_x)\n",
        "            loss_s = F.cross_entropy(support_logits, support_y)\n",
        "            learner.adapt(loss_s)\n",
        "\n",
        "            # Test on query set\n",
        "            with torch.no_grad():\n",
        "                query_logits = learner(query_x)\n",
        "                preds = query_logits.argmax(dim=1)\n",
        "                acc = (preds == query_y).float().mean().item()\n",
        "                test_accuracies.append(acc)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in test task {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if test_accuracies:\n",
        "        mean_acc = np.mean(test_accuracies)\n",
        "        std_acc = np.std(test_accuracies)\n",
        "        print(f\"Test Performance: {mean_acc:.3f} ± {std_acc:.3f}\")\n",
        "        return mean_acc, std_acc\n",
        "    else:\n",
        "        print(\"No successful test tasks!\")\n",
        "        return 0.0, 0.0\n",
        "\n",
        "# evaluate_few_shot(maml, task_sampler)"
      ],
      "metadata": {
        "id": "Iruylxk_z3Cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9c0eb9d-2635-4c20-e15e-42df03167342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 classes with samples: [487, 142, 28]\n",
            "Starting meta-training with custom MAML implementation...\n",
            "Epoch  100  Loss 1.0650  Acc 0.433  MAE 0.6333  MSE 0.7667  RMSE 0.8756  R² -0.1500\n",
            "Epoch  200  Loss 1.1031  Acc 0.300  MAE 0.7333  MSE 0.8000  RMSE 0.8944  R² -0.2000\n",
            "Epoch  300  Loss 1.1237  Acc 0.367  MAE 0.7000  MSE 0.8333  RMSE 0.9129  R² -0.2500\n",
            "Epoch  400  Loss 1.1152  Acc 0.367  MAE 0.7333  MSE 0.9333  RMSE 0.9661  R² -0.4000\n",
            "Epoch  500  Loss 1.1002  Acc 0.300  MAE 0.8667  MSE 1.2000  RMSE 1.0954  R² -0.8000\n",
            "Epoch  600  Loss 1.1109  Acc 0.300  MAE 0.8333  MSE 1.1000  RMSE 1.0488  R² -0.6500\n",
            "Epoch  700  Loss 1.1371  Acc 0.367  MAE 0.6667  MSE 0.7333  RMSE 0.8563  R² -0.1000\n",
            "Epoch  800  Loss 1.1493  Acc 0.267  MAE 0.8000  MSE 0.9333  RMSE 0.9661  R² -0.4000\n",
            "Epoch  900  Loss 1.1054  Acc 0.500  MAE 0.5000  MSE 0.5000  RMSE 0.7071  R² 0.2500\n",
            "Epoch 1000  Loss 1.0886  Acc 0.300  MAE 0.8333  MSE 1.1000  RMSE 1.0488  R² -0.6500\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few‑Shot Linear‑Probe (1‑line classification head) - decided not to use in the end\n"
      ],
      "metadata": {
        "id": "VDkoXJhYz9cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "weights = ViT_B_16_Weights.DEFAULT\n",
        "backbone = vit_b_16(weights=weights)\n",
        "backbone.heads = nn.Identity()       # remove classifier\n",
        "for p in backbone.parameters():      # freeze\n",
        "    p.requires_grad = False\n",
        "\n",
        "\n",
        "few_ds = ...  # subset AdImageDataset with 15 images total\n",
        "few_loader = DataLoader(few_ds, batch_size=15)\n",
        "\n",
        "# extract embeddings (one batch)\n",
        "imgs, lbs = next(iter(few_loader))\n",
        "embs = backbone(imgs.to(device)).cpu()\n",
        "\n",
        "# train logistic regression on these 15 vectors\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(max_iter=1000).fit(embs, lbs)\n",
        "\n",
        "# inference\n",
        "val_ds  = AdImageDataset(r'C:\\Bakalauras\\downloaded_images\\val', img_tf)\n",
        "val_emb = []\n",
        "val_lab = []\n",
        "for img,l in DataLoader(val_ds, batch_size=32):\n",
        "    with torch.no_grad():\n",
        "        val_emb.append(backbone(img.to(device)).cpu())\n",
        "        val_lab.append(l)\n",
        "val_emb = torch.cat(val_emb).numpy(); val_lab = torch.cat(val_lab).numpy()\n",
        "pred = clf.predict(val_emb)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(val_lab, pred, digits=4))\n"
      ],
      "metadata": {
        "id": "hAiRvPc3z-M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# 1. Imports (torch 1.8.1, torchvision 0.9.1, torchmeta 1.7.0)\n",
        "# ==========================================================\n",
        "import os, glob, re, random\n",
        "from PIL import Image\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset\n",
        "from torchmeta.utils.data import (\n",
        "    ClassDataset, CombinationMetaDataset, BatchMetaDataLoader,\n",
        "    Dataset as MetaDataset,\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2. Simple (ctr, impressions) → class label\n",
        "# ----------------------------------------------------------\n",
        "def to_label(ctr, impr):\n",
        "    if impr < 1e5 and ctr < 0.10:\n",
        "        return 0          # low\n",
        "    if impr >= 3e5 or ctr >= 0.20:\n",
        "        return 2          # high\n",
        "    return 1              # medium\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3. Plain banner-image dataset\n",
        "# ----------------------------------------------------------\n",
        "class AdImageDataset(Dataset):\n",
        "    _pat = re.compile(r'([\\d.]+)_([\\d]+)_.+\\.jpg$', re.I)\n",
        "\n",
        "    def __init__(self, root, transform):\n",
        "        self.transform = transform\n",
        "        self.samples   = []               # (path, lbl)\n",
        "        for p in glob.glob(os.path.join(root, '*.jpg')):\n",
        "            m = self._pat.search(os.path.basename(p))\n",
        "            if not m:\n",
        "                continue\n",
        "            ctr, impr = float(m.group(1)), float(m.group(2))\n",
        "            lbl = to_label(ctr, impr)\n",
        "            self.samples.append((p, lbl))\n",
        "        self.labels  = [lbl for _, lbl in self.samples]  # torchmeta looks here\n",
        "        self.targets = self.labels\n",
        "\n",
        "    def __len__(self):  return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, lbl = self.samples[idx]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        return img, lbl\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4. Per-class dataset that torchmeta 1.7.0 can use\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "from torchmeta.utils.data import Dataset as MetaDataset\n",
        "\n",
        "class ClassImagesDataset(MetaDataset):\n",
        "    \"\"\"Dataset containing all images of ONE class.\"\"\"\n",
        "    def __init__(self, paths, transform, cls_idx):\n",
        "        # supply required positional index and the transform\n",
        "        super().__init__(cls_idx, transform=transform, meta_split='train')\n",
        "        self.paths = paths            # list of file paths\n",
        "        self.index = cls_idx          # class label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        return img, self.index\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 5. ClassDataset wrapper\n",
        "# ----------------------------------------------------------\n",
        "class AdClassDataset(ClassDataset):\n",
        "    def __init__(self, root, transform):\n",
        "        # tell torchmeta this is the meta-training split\n",
        "        super().__init__(meta_train=True)\n",
        "\n",
        "        base = AdImageDataset(root, transform)\n",
        "        self.cls_paths = {0: [], 1: [], 2: []}\n",
        "        for p, l in base.samples:\n",
        "            self.cls_paths[l].append(p)\n",
        "\n",
        "        self.classes   = list(self.cls_paths.keys())\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):  return len(self.classes)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cls = self.classes[idx]\n",
        "        return ClassImagesDataset(self.cls_paths[cls], self.transform, cls)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 6. Image transforms\n",
        "# ----------------------------------------------------------\n",
        "img_tf = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# ==========================================================\n",
        "# 7. Meta-dataset & loader (num_workers=0 on Windows)\n",
        "# ==========================================================\n",
        "root_dir = r'C:\\Bakalauras\\downloaded_images'\n",
        "N_way, K_shot, Q_query = 3, 5, 10\n",
        "\n",
        "cls_dataset = AdClassDataset(root_dir, img_tf)\n",
        "meta_ds = CombinationMetaDataset(cls_dataset,\n",
        "                                 num_classes_per_task=N_way)\n",
        "\n",
        "loader = BatchMetaDataLoader(meta_ds,\n",
        "                             batch_size=1,\n",
        "                             shuffle=True,\n",
        "                             num_workers=0)\n",
        "\n",
        "# ==========================================================\n",
        "# 8. ProtoNet backbone (torchvision 0.9.1)\n",
        "# ==========================================================\n",
        "class ProtoNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet18(pretrained=True)  # old API\n",
        "        self.backbone.fc = nn.Identity()                  # 512-d output\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model  = ProtoNet().to(device)\n",
        "optim  = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# ==========================================================\n",
        "# 9. Episodic training loop\n",
        "# ==========================================================\n",
        "def load_stack(ds, idxs):\n",
        "    return torch.stack([ds[i][0] for i in idxs])\n",
        "\n",
        "for epoch in range(5):\n",
        "    for episode in loader:                   # episode = ConcatTask\n",
        "        support_x, query_x = [], []\n",
        "        support_y, query_y = [], []\n",
        "\n",
        "        # build support/query sets\n",
        "        ok = True\n",
        "        for cls_idx, class_ds in enumerate(episode.datasets):\n",
        "            if len(class_ds) < K_shot + Q_query:\n",
        "                ok = False; break\n",
        "            perm = torch.randperm(len(class_ds))\n",
        "            sup_idx = perm[:K_shot]\n",
        "            qry_idx = perm[K_shot:K_shot+Q_query]\n",
        "\n",
        "            support_x.append(load_stack(class_ds, sup_idx))\n",
        "            query_x.append(load_stack(class_ds, qry_idx))\n",
        "            support_y += [cls_idx] * K_shot\n",
        "            query_y   += [cls_idx] * Q_query\n",
        "\n",
        "        if not ok:\n",
        "            continue\n",
        "\n",
        "        support_x = torch.cat(support_x).to(device)\n",
        "        query_x   = torch.cat(query_x).to(device)\n",
        "        support_y = torch.tensor(support_y, device=device)\n",
        "        query_y   = torch.tensor(query_y,   device=device)\n",
        "\n",
        "        # embeddings\n",
        "        emb_sup = model(support_x)\n",
        "        emb_qry = model(query_x)\n",
        "\n",
        "        # prototypes\n",
        "        protos = torch.stack([emb_sup[support_y==c].mean(0)\n",
        "                              for c in range(N_way)])\n",
        "\n",
        "        # distances & loss\n",
        "        logits = -((emb_qry.unsqueeze(1) - protos)**2).sum(-1)\n",
        "        loss   = F.cross_entropy(logits, query_y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}: episode loss {loss.item():.4f}')\n"
      ],
      "metadata": {
        "id": "MjhzI9-I0Gea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmentations Experiment\n"
      ],
      "metadata": {
        "id": "pC35BBkpWFRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, re\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.metrics import classification_report, mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# 1) Dataset for 3-way classification (viral categories)\n",
        "class ViralAdsDataset(Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        pattern = re.compile(r\"^([\\d.]+)_([\\d]+)_.+\\.jpg$\")\n",
        "        for path in glob.glob(os.path.join(folder, \"*.jpg\")):\n",
        "            fn = os.path.basename(path)\n",
        "            m = pattern.match(fn)\n",
        "            if not m:\n",
        "                continue\n",
        "            ctr = float(m.group(1))\n",
        "            impr = float(m.group(2))\n",
        "            # label: 0=no,1=moderate,2=viral\n",
        "            if impr < 1e5 and ctr < 0.1:\n",
        "                label = 0\n",
        "            elif impr >= 3e5 or ctr >= 0.2:\n",
        "                label = 2\n",
        "            else:\n",
        "                label = 1\n",
        "            self.samples.append((path, label))\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# 2) Strong augmentations for training, simple for validation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "# 3) Prepare data loaders\n",
        "dataset = ViralAdsDataset(r\"/content/sample_data\", transform=None)\n",
        "n_train = int(0.8 * len(dataset))\n",
        "n_val = len(dataset) - n_train\n",
        "train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
        "train_ds.dataset.transform = train_transform\n",
        "val_ds.dataset.transform = val_transform\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# 4) Model: ResNet-18 fine-tuning head only\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.resnet18(pretrained=True)\n",
        "for name,param in model.named_parameters():\n",
        "    if not name.startswith(\"fc\"):\n",
        "        param.requires_grad = False\n",
        "model.fc = nn.Linear(model.fc.in_features, 3)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "\n",
        "# 5) Training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * imgs.size(0)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}  Train Loss: {total_loss/len(train_loader.dataset):.4f}\")\n",
        "\n",
        "# 6) Evaluation\n",
        "model.eval()\n",
        "all_preds, all_trues = [], []\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        logits = model(imgs)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_trues.extend(labels.numpy())\n",
        "\n",
        "# 7) Metrics\n",
        "print(classification_report(all_trues, all_preds, digits=4))\n",
        "mae  = mean_absolute_error(all_trues, all_preds)\n",
        "mse  = mean_squared_error(all_trues, all_preds)\n",
        "rmse = np.sqrt(mse)\n",
        "r2   = r2_score(all_trues, all_preds)\n",
        "\n",
        "print(f\"MAE:  {mae:.4f}\")\n",
        "print(f\"MSE:  {mse:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"R²:   {r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGaKxtD8WKr7",
        "outputId": "35b225b9-ef17-4d65-c61b-7f1d5a6df449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 105MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20  Train Loss: 1.1850\n",
            "Epoch 2/20  Train Loss: 0.8347\n",
            "Epoch 3/20  Train Loss: 0.7534\n",
            "Epoch 4/20  Train Loss: 0.7223\n",
            "Epoch 5/20  Train Loss: 0.7046\n",
            "Epoch 6/20  Train Loss: 0.6964\n",
            "Epoch 7/20  Train Loss: 0.6965\n",
            "Epoch 8/20  Train Loss: 0.6749\n",
            "Epoch 9/20  Train Loss: 0.6679\n",
            "Epoch 10/20  Train Loss: 0.6592\n",
            "Epoch 11/20  Train Loss: 0.6561\n",
            "Epoch 12/20  Train Loss: 0.6516\n",
            "Epoch 13/20  Train Loss: 0.6472\n",
            "Epoch 14/20  Train Loss: 0.6349\n",
            "Epoch 15/20  Train Loss: 0.6284\n",
            "Epoch 16/20  Train Loss: 0.6290\n",
            "Epoch 17/20  Train Loss: 0.6218\n",
            "Epoch 18/20  Train Loss: 0.6151\n",
            "Epoch 19/20  Train Loss: 0.6058\n",
            "Epoch 20/20  Train Loss: 0.6083\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000         5\n",
            "           1     0.4444    0.1905    0.2667        21\n",
            "           2     0.8211    0.9528    0.8821       106\n",
            "\n",
            "    accuracy                         0.7955       132\n",
            "   macro avg     0.4219    0.3811    0.3829       132\n",
            "weighted avg     0.7301    0.7955    0.7508       132\n",
            "\n",
            "MAE:  0.2424\n",
            "MSE:  0.3182\n",
            "RMSE: 0.5641\n",
            "R²:   -0.2456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}